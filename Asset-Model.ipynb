{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2019  Communication Network Security Laboratory, SSU, Seoul\n",
    "\n",
    "# This program is free software; you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation; either version 2 of the License, or\n",
    "# (at your option) any later version.\n",
    "\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e552010458bf65ffbbdeebcc498831c3c6cc85e"
   },
   "source": [
    "# LibAndAssets\n",
    "\n",
    "## What is this code ? \n",
    "This code are used to generate the list of packer signature based on mutual_dict.\n",
    "\n",
    "To the effective of our method, applied the train/test packer dataset that was splitted by DEX Analysis experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "72ae04117f20c463a1633ce53b10f425281711f2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG ARGURMENTS\n",
    "ASSETS_DIR = \"dataset/kaggle/assets/\"\n",
    "BASE_DIR = \"dataset/kaggle/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05a9a9192929db62e3ec2b1992332ddcf153bf57"
   },
   "source": [
    "# STEP 1: COLLECT DATASET AND TRANSFORM\n",
    "\n",
    "THIS STEP HAS BEEN RUN AND EXPORTED AS PICKLE FILE. CHANGE THE MARKDOWN SECTIONS THAT COME WITH \"CODE\" FOR RERUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "56296ce39004c8175715fc88cda574ece7e6ea91"
   },
   "outputs": [],
   "source": [
    "### CODE - DEFINE CLASS \n",
    "class Assets_FeatureModel(object):\n",
    "    \"\"\"Model for classification.\"\"\"\n",
    "    def commonizePacker(self, df):                                                                                                                                                                                                                                                                                                                                                                        \n",
    "        df['packerizer'] = df['packer'].apply(self.packerlizer)                      \n",
    "        return df\n",
    "    \n",
    "    def hasExt(self, x):\n",
    "        if \".\" in x:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def hasValidData(self, x):\n",
    "        if \".\" in x:\n",
    "            valid_data = ['data', 'so', 'jar', \n",
    "                          'dat', 'bin', 'zip',\n",
    "                          'jpg'\n",
    "                         ]\n",
    "            ext = x.split(\".\")[-1]\n",
    "            for d in valid_data:\n",
    "                if d == ext:\n",
    "                    return True\n",
    "            return False\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def hasValidDataType(self, x):\n",
    "        valid_data = ['data', 'elf', 'zip']\n",
    "        print(x)\n",
    "        for d in valid_data:\n",
    "            if d == x:\n",
    "                return True\n",
    "        return False\n",
    "              \n",
    "    def refineDF(self, df):\n",
    "        #df = df[~df['name'].str.contains(\".png\")]\n",
    "        #df = df[~df['name'].str.contains(\".jpg\")]\n",
    "        #df = df[~df['name'].str.contains(\".ogg\")]\n",
    "        #df = df[~df['name'].str.contains(\".RSA\")]\n",
    "        #df = df[~df['name'].str.contains(\".lua\")]\n",
    "        #df = df[~df['name'].str.contains(\".xml\")]\n",
    "        #df = df[~df['name'].str.contains(\".obm\")]\n",
    "        #df = df[~df['name'].str.contains(\".btb\")]\n",
    "        # df = df[df['type'].apply(self.hasValidDataType) == True] # BAD MODEL\n",
    "        df = df[df['name'].apply(self.hasValidData) == True]\n",
    "        #df = df[df['name'].apply(self.hasExt) == True]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def vector_hash(self, x):\n",
    "        hash_x = hash(\"-\".join(str(e) for e in x))\n",
    "        return hash_x\n",
    "        \n",
    "    def createHashVector(self, df):\n",
    "        df['vector_hash'] = df['vector'].apply(self.vector_hash)      \n",
    "        return df\n",
    "    \n",
    "    def packerlizer(self, x):\n",
    "        unique_packers = [                                                                              \n",
    "            'Ijiami', 'Bangcle', 'Qihoo 360', 'APKProtect',                                             \n",
    "            'Jiagu'                                                                                   \n",
    "        ]\n",
    "        for i in unique_packers:\n",
    "            if i in x:\n",
    "                return i\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9caf50da23c50f21912c0872265e5f2ad0687306"
   },
   "source": [
    "GET DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "db3209c1e65ddb9d327725f85ceaa3190847f21b"
   },
   "outputs": [],
   "source": [
    "# CODE - THIS STEP COLLECT PACKER AND NORMAL DATASET WHICH COLLECT BY OUR SCRIPT (PRE-PROCESSING STAGE)\n",
    "\n",
    "packer_df = pd.DataFrame()\n",
    "normal_df = pd.DataFrame()\n",
    "for f in os.listdir(ASSETS_DIR):\n",
    "    if all(x in f for x in ['df_assets_packer']):\n",
    "        df = pd.read_json(os.path.join(ASSETS_DIR, f))\n",
    "        packer_df = packer_df.append(df)\n",
    "    if all(x in f for x in ['df_assets_normal']):\n",
    "        df = pd.read_json(os.path.join(ASSETS_DIR, f))\n",
    "        normal_df = normal_df.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE - COMBINED WITH DATASET FROM DEX ANALYSIS\n",
    "\n",
    "#### INTEGRATED ARE BASED ON DEX ANALYSIS DATASET (ALREADY UNDER-SAMPLING)\n",
    "INTEGRATED_FILE = os.path.join(BASE_DIR,\"integrated_df.pickle\") \n",
    "integrated_df = pd.read_pickle(INTEGRATED_FILE)\n",
    "saved_packer_df = integrated_df[integrated_df['packer_label'] != 0]\n",
    "\n",
    "##### GET Intersect of md5\n",
    "md5_packer = np.intersect1d(saved_packer_df.md5, packer_df.app_md5).tolist()\n",
    "print(\"Number of packer in test: %d\" % len(md5_packer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE - CHECK FOR PACKER ASSETS AND MAKE NEW PACKER DF (SAME)\n",
    "\n",
    "#### Make NEW FOR IMPLEMENTATIONS, WE DON\"T NEED TO SPLIT ANYMORE\n",
    "i_packer_df = pd.DataFrame()\n",
    "i_normal_df = normal_df\n",
    "\n",
    "for md5 in md5_packer:\n",
    "    i_packer_df = i_packer_df.append(packer_df[packer_df['app_md5'] == md5], ignore_index=True)\n",
    "    \n",
    "print(\"Number of packer assets in test: %d\" % len(i_packer_df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming - Reducing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6a109991be2cc11e750345254a2ce8f125c7eb5"
   },
   "source": [
    "Create hashes vector based on the packers information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f325662ae3dcf98ed347f4f8e9f1766e8ad0a9b8"
   },
   "source": [
    "# CODE - TRANSFORM THE PACKER DATASET\n",
    "\n",
    "# Packerizer and extract valid data\n",
    "#feature_model = Assets_FeatureModel()\n",
    "#i_packer_df = feature_model.refineDF(i_packer_df)\n",
    "#i_packer_df = feature_model.commonizePacker(i_packer_df)\n",
    "#i_packer_df = feature_model.createHashVector(i_packer_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "606fdd71dcd415cc37c260ccd148c74aaccb14eb"
   },
   "source": [
    "# DEBUG\n",
    "\n",
    "##### Unique_signature, Test\n",
    "dup_sig = i_packer_df[i_packer_df['vector_hash'].duplicated()]\n",
    "uniq_sig = i_packer_df['vector_hash'].unique()\n",
    "final_packer_df = pd.DataFrame()\n",
    "\n",
    "#### TAKE A LONG TIME FOR THIS \n",
    "for sig in uniq_sig:\n",
    "    # GET ONE SAMPLE FROM A VECTOR_HASH TO CALCULATE\n",
    "    final_packer_df = final_packer_df.append(\n",
    "        i_packer_df[i_packer_df['vector_hash'] == sig].iloc[0], ignore_index=True)\n",
    "        \n",
    "final_packer_assets = os.path.join(ASSETS_DIR, \"final_packer_assets_27012019.pickle\")\n",
    "final_packer_df.to_pickle(final_packer_assets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEBUG - SAVE THE FULL PACKER DATASET TO FILE\n",
    "\n",
    "#WRONG CODE HERE!, THE ASSETS HAVE TO BE REDUCED AFTER SPLITTED. WE REDUCED THE ASSETS TO IMPROVE TRAINING PROCESS. \n",
    "final_packer_assets = os.path.join(ASSETS_DIR, \"final_packer_assets_24012019.pickle\")\n",
    "final_packer_assets_df = pd.read_pickle(final_packer_assets)\n",
    "#END WRONGCODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE - Collect packer df from stored file\n",
    "final_packer_df_path = os.path.join(ASSETS_DIR, \"final_packer_df_24012019.pickle\")\n",
    "i_packer_df.to_pickle(final_packer_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "b11cad364ba2897f38539e523bf7e79913a26db7"
   },
   "outputs": [],
   "source": [
    "final_packer_df_path = os.path.join(ASSETS_DIR, \"final_packer_assets_27012019.pickle\")\n",
    "i_packer_df = pd.read_pickle(final_packer_df_path)\n",
    "\n",
    "# Collect normal df from stored file\n",
    "final_normal_df = pd.DataFrame()\n",
    "for f in os.listdir(ASSETS_DIR):\n",
    "    if all(x in f for x in ['df_assets_normal']):\n",
    "        df = pd.read_json(os.path.join(ASSETS_DIR, f))\n",
    "        final_normal_df = final_normal_df.append(df)\n",
    "        \n",
    "# ax = sns.countplot(y=\"\", data=final_packer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  COLLECT THE REAL TRAIN & TEST PACKER DATASET BASED ON MD5\n",
    "\n",
    "Get list of train/test packer dataset based on md5 list that shared by DEX analysis process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "ef1d32cc0378d6b9fcd85f52d7f4c552d6abbfdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total packer for test: 196\n",
      "Total packer for train: 286\n",
      "Total test packer after refining: 195\n",
      "Total train packer after refining: 284\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH SHARED\n",
    "# SPLITTED_PACKER_FILE = os.path.join(BASE_DIR,\"splited_packer_df.pickle\")\n",
    "# splited_packer = pd.read_pickle(SPLITTED_PACKER_FILE)\n",
    "\n",
    "if os.path.isfile(os.path.join(BASE_DIR, \"train_packer_md5.pickle\")):\n",
    "    md5_train = pd.read_pickle(os.path.join(BASE_DIR, \"train_packer_md5.pickle\"))\n",
    "if os.path.isfile(os.path.join(BASE_DIR, \"test_packer_md5.pickle\")):\n",
    "    md5_test = pd.read_pickle(os.path.join(BASE_DIR, \"test_packer_md5.pickle\"))\n",
    "    \n",
    "md5_train.columns=['app_md5', 'packer']\n",
    "md5_test.columns=['app_md5', 'packer']\n",
    "\n",
    "merged_train_df = pd.merge(packer_df, md5_train, how='inner', on=['app_md5']) # Merged to final train set\n",
    "merged_test_df = pd.merge(packer_df, md5_test, how='inner', on=['app_md5']) # Merged to final test set\n",
    "\n",
    "print(\"Total packer for test: %d\" % len(merged_test_df.app_md5.unique()))\n",
    "print(\"Total packer for train: %d\" % len(merged_train_df.app_md5.unique()))\n",
    "\n",
    "\n",
    "\n",
    "# Calculate vector_hash\n",
    "feature_model = Assets_FeatureModel()\n",
    "\n",
    "merged_test_df = merged_test_df.drop(['packer_y'], axis=1)\n",
    "merged_test_df = merged_test_df.rename(index=str, columns={\"packer_x\": \"packer\"})\n",
    "merged_test_df = feature_model.refineDF(merged_test_df)\n",
    "merged_test_df = feature_model.commonizePacker(merged_test_df)\n",
    "merged_test_df = feature_model.createHashVector(merged_test_df)\n",
    "\n",
    "merged_train_df = merged_train_df.drop(['packer_y'], axis=1)\n",
    "merged_train_df = merged_train_df.rename(index=str, columns={\"packer_x\": \"packer\"})\n",
    "merged_train_df = feature_model.refineDF(merged_train_df)\n",
    "merged_train_df = feature_model.commonizePacker(merged_train_df)\n",
    "merged_train_df = feature_model.createHashVector(merged_train_df)\n",
    "\n",
    "print(\"Total test packer after refining: %d\" % len(merged_test_df.app_md5.unique()))\n",
    "print(\"Total train packer after refining: %d\" % len(merged_train_df.app_md5.unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After Refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test assets: 7599\n",
      "Total train assets: 13807\n"
     ]
    }
   ],
   "source": [
    "print(\"Total test assets: %d\" % len(merged_test_df.index))\n",
    "print(\"Total train assets: %d\" % len(merged_train_df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOW, GET THE UNIQUE LIST OF ASSETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before remove duplicated: 7599\n",
      "After remove duplicated: 1347\n"
     ]
    }
   ],
   "source": [
    "#uniq_sig = pd.DataFrame(columns=['vector_hash'])\n",
    "#uniq_sig['vector_hash'] = merged_train_df['vector_hash'].unique()\n",
    "#reduced_ptrain_df = pd.merge(merged_train_df, uniq_sig, how='inner', on=['vector_hash'])\n",
    "\n",
    "reduced_test_df = merged_test_df.drop_duplicates(subset='vector_hash', keep=\"first\")\n",
    "print(\"Before remove duplicated: %d\" % len(merged_test_df['vector_hash'].index))\n",
    "print(\"After remove duplicated: %d\" % len(reduced_test_df['vector_hash'].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT, CALCULATE THE MUTUAL INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "99ce0d88cdd353de0448f137d19a44c07c17bc0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for packer: Unicom SDK Loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 1/11 [00:04<00:42,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for packer: Ijiami\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 2/11 [00:05<00:31,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for packer: Qihoo 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 3/11 [00:07<00:22,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for packer: Bangcle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▋      | 4/11 [00:08<00:16,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for packer: Jiagu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 5/11 [00:10<00:13,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for packer: APKProtect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▍    | 6/11 [00:11<00:09,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for packer: Tencent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▎   | 7/11 [00:12<00:06,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for packer: SecNeo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 8/11 [00:13<00:03,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for packer: DexProtector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 9/11 [00:13<00:02,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for packer: Naga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 10/11 [00:14<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for packer: UPX (unknown, modified)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 11/11 [00:15<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapse time: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "import pickle\n",
    "\n",
    "def _createTask(train, alg, p, threshold):\n",
    "    print(\"Training for packer: %s\" % p) \n",
    "    Xtrain = []\n",
    "    ytrain = []\n",
    "    p_df = train[train['packerizer'] == p]\n",
    "    collect_num = len(p_df.index)\n",
    "    if collect_num < 10:\n",
    "        return\n",
    "    for index, row in p_df.iterrows():\n",
    "        counts = Counter(row['vector'])\n",
    "        fvector = dict(counts)\n",
    "        Xtrain.append(fvector)\n",
    "        ytrain.append(row['packer'])\n",
    "    dv = DictVectorizer()\n",
    "    X_vec = dv.fit_transform(Xtrain)\n",
    "    feature_scores = mutual_info_classif(X_vec, ytrain)\n",
    "    if alg == \"chi\":\n",
    "        feature_scores = chi2(X_vec, ytrain)[0]\n",
    "    if alg == \"f_classif\":\n",
    "        feature_scores = f_classif(X_vec, ytrain)[0]\n",
    "    names = []\n",
    "    for score, fname in sorted(zip(feature_scores, dv.get_feature_names()), reverse=True)[:threshold]:\n",
    "        names.append(fname)\n",
    "    return names\n",
    "\n",
    "def createMutualDict(train, alg=\"mi\", threshold=10):\n",
    "    import pickle\n",
    "    if os.path.isfile(os.path.join(BASE_DIR, \"mutual_dict_t%d.pickle\" % threshold)):\n",
    "        return pickle.load(open(os.path.join(BASE_DIR, \"mutual_dict_t%d.pickle\" % threshold), \"rb\" ))\n",
    "    from tqdm import tqdm\n",
    "    import multiprocessing\n",
    "    mutual_dict = {}\n",
    "    total_packer = len(train.packerizer.unique())\n",
    "    pbar = tqdm(total=total_packer)\n",
    "    for p in train.packerizer.unique():\n",
    "        result = _createTask(train, alg, p, threshold)\n",
    "        mutual_dict[p] = result\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    pickle.dump(mutual_dict, open( \"mutual_dict_t%d.pickle\" % threshold, \"wb\" ))\n",
    "    return mutual_dict\n",
    "start_time = time.time()\n",
    "mutual_dict = createMutualDict(merged_train_df, \"mi\")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapse time: %d\" % elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "f1bf65049975263ac38ca2f62fc25960b6aae31b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Naga': [108, 965, 960, 955, 953, 951, 950, 948, 947, 945], 'Bangcle': [108, 676, 813, 111, 100, 841, 184, 843, 787, 57], 'SecNeo': [957, 951, 946, 944, 934, 932, 930, 927, 925, 921], 'Tencent': [947, 714, 918, 745, 583, 581, 491, 511, 488, 211], 'Qihoo 360': [799, 348, 122, 114, 967, 811, 804, 792, 112, 257], 'DexProtector': [915, 110, 930, 921, 118, 45, 971, 923, 918, 916], 'Ijiami': [97, 51, 50, 46, 55, 56, 54, 53, 52, 103], 'UPX (unknown, modified)': [918, 888, 877, 870, 862, 855, 854, 849, 847, 828], 'Unicom SDK Loader': [136, 126, 80, 72, 179, 93, 978, 333, 152, 403], 'Jiagu': [95, 119, 106, 100, 112, 103, 108, 98, 105, 46], 'APKProtect': [958, 934, 965, 453, 946, 939, 917, 643, 931, 922]}\n"
     ]
    }
   ],
   "source": [
    "# Save mutual dict\n",
    "print(mutual_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "e489466f4263e3779b04c1201a0a6a3577229788"
   },
   "outputs": [],
   "source": [
    "import queue\n",
    "import threading\n",
    "\n",
    "class MultiTask(object):\n",
    "    \"\"\"For running multi-task.\"\"\"\n",
    "\n",
    "    def __init__(self, task_name, thread_num=10):\n",
    "        \"\"\"Init.\"\"\"\n",
    "        self.task_name = task_name\n",
    "        self.tnum = thread_num\n",
    "\n",
    "    def run(self, input_list):\n",
    "        \"\"\"Run task.\"\"\"\n",
    "        work = queue.Queue()\n",
    "        results = queue.Queue()\n",
    "\n",
    "        for i in range(self.tnum):\n",
    "            t = threading.Thread(target=self.task_name,\n",
    "                                 args=(work, results))\n",
    "            t.daemon = True\n",
    "            t.start()\n",
    "\n",
    "        i = 0\n",
    "        for i in range(len(input_list)):\n",
    "            work.put(input_list[i])\n",
    "\n",
    "        work.join()\n",
    "\n",
    "        i = 0\n",
    "        out = []\n",
    "        for i in range(len(input_list)):\n",
    "            res = results.get()\n",
    "            out.append(res)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "d1dd8aa6f507b1d70fbb70621add9a0b907701c6"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "CACHE_APP_DIR = os.path.join(ASSETS_DIR, \"cache\")\n",
    "\n",
    "class MutualInformationClassification(object):\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.appCache = {}\n",
    "        self.cacheFlg = 1\n",
    "        self.checkType = 1 # 0 is assets_check, 1 is app_check\n",
    "        \n",
    "    def setCache(self):\n",
    "        self.cacheFlg = 1\n",
    "        return\n",
    "    \n",
    "    def unsetCache(self):\n",
    "        self.cacheFlg = 0\n",
    "        return\n",
    "    \n",
    "    def getCache(self):\n",
    "        return self.cache\n",
    "    \n",
    "    def setCache(self, json):\n",
    "        self.cache = json\n",
    "        return \n",
    "    \n",
    "    def clearCache(self):\n",
    "        self.cache = {}\n",
    "        return\n",
    "\n",
    "    def storeCache(self):\n",
    "        # Store cache result to json\n",
    "        last_num = 0\n",
    "        for f in os.listdir(ASSETS_DIR):\n",
    "            if \"cache_assets\" in f:\n",
    "                # Store cache_assets named with: cache_assets_<num>\n",
    "                cache_num = int(f.split(\"_\")[-1])\n",
    "                if cache_num > last_num:\n",
    "                    last_num = cache_num\n",
    "\n",
    "        last_num += 1\n",
    "        saved = os.path.join(ASSETS_DIR, \"cache_assets_%d\" % last_num)\n",
    "        with open(saved, 'w') as fp:\n",
    "            json.dump(self.cache, fp)\n",
    "        self.clearCache()\n",
    "        \n",
    "    def loadAppCache(self, md5):\n",
    "        data = {}\n",
    "        cache_app = \"cache_app_%s\" % md5\n",
    "        for f in os.listdir(CACHE_APP_DIR):\n",
    "            if cache_app in f:\n",
    "                saved = os.path.join(CACHE_APP_DIR, f)\n",
    "                with open(saved, 'r') as fp:\n",
    "                    ds = json.load(fp)\n",
    "                    data.update(ds)\n",
    "        self.appCache = data\n",
    "        print(\"Loaded %d vector hash for app: %s \" % (len(data), md5))\n",
    "        return self.appCache\n",
    "    \n",
    "    def storeAppCache(self, md5):\n",
    "        # Store cache result to json\n",
    "        saved = os.path.join(CACHE_APP_DIR, \"cache_app_%s\" % md5)\n",
    "        if not os.path.isfile(saved):\n",
    "            return\n",
    "        with open(saved, 'w') as fp:\n",
    "            json.dump(self.appCache, fp)\n",
    "        print(\"Stored %d vector hash to app: %s \" % (len(self.appCache), md5))\n",
    "        self.clearAppCache()\n",
    "            \n",
    "    def clearAppCache(self):\n",
    "        self.appCache = {}\n",
    "        return\n",
    "    \n",
    "    def loadCache(self):\n",
    "        data = {}\n",
    "        for f in os.listdir(ASSETS_DIR):\n",
    "            if \"cache_assets\" in f:\n",
    "                saved = os.path.join(ASSETS_DIR, f)\n",
    "                with open(saved, 'r') as fp:\n",
    "                    ds = json.load(fp)\n",
    "                    data.update(ds)\n",
    "        self.cache = data\n",
    "        print(\"Loaded %d vector hash\" % len(data))\n",
    "        return self.cache\n",
    "        \n",
    "    def taskMiCal(self, in_queue, out_queue):\n",
    "        \"\"\"Threading work.\"\"\"\n",
    "        while True:\n",
    "            inp = in_queue.get()\n",
    "            # process\n",
    "            result = self.taskMiCal_t(inp)\n",
    "            out_queue.put(result)\n",
    "            in_queue.task_done()\n",
    "        return None\n",
    "    \n",
    "    def taskMiCal_t(self, input_list):\n",
    "        fvector = input_list['fvector']\n",
    "        packer = input_list['packer']\n",
    "        tested  = input_list['tested']\n",
    "        m = input_list['m'] \n",
    "        threshold = input_list['threshold']\n",
    "        ytrain = []\n",
    "        ytrain.append(tested)\n",
    "        comXTrain = []\n",
    "        comXTrain.append(fvector)\n",
    "        counts2 = Counter(m)\n",
    "        fvector2 = dict(counts2)\n",
    "        comXTrain.append(fvector2)\n",
    "        ytrain.append(packer)\n",
    "        dv = DictVectorizer()\n",
    "        X_vec = dv.fit_transform(comXTrain)\n",
    "        feature_scores = mutual_info_classif(X_vec, ytrain)\n",
    "        matched = 0\n",
    "        for score, fname in sorted(zip(feature_scores, dv.get_feature_names()), reverse=True)[:threshold]:\n",
    "            if score != 0.6931471805599453:\n",
    "                matched += 1\n",
    "        if matched > 0:            \n",
    "            return packer\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def checkRow(self, app_df, mutual_dict, threshold):\n",
    "        # import multiprocessing\n",
    "        # pool = multiprocessing.Pool(processes =  len(mutual_dict))\n",
    "        output = []\n",
    "        asset_vector = []\n",
    "        packer_vector = []\n",
    "        type_vector = []\n",
    "        print(\"Check %d assets from app\" % len(app_df.index))\n",
    "        for index, row in app_df.iterrows():\n",
    "            # Check hashes\n",
    "            data_type = row['type']\n",
    "            assets = row['name']\n",
    "            if self.cacheFlg:\n",
    "                if row['vector_hash'] in self.cache:\n",
    "                    predict = self.cache[row['vector_hash']]\n",
    "                    output.append(\"%s:%s:%s:%s\" % (assets, data_type, predict, row['vector_hash']))\n",
    "                    continue\n",
    "                if row['vector_hash'] in self.appCache:\n",
    "                    predict = self.appCache[row['vector_hash']]\n",
    "                    output.append(\"%s:%s:%s:%s\" % (assets, data_type, predict, row['vector_hash']))\n",
    "                    continue\n",
    "            tested = row['packer']\n",
    "            counts = Counter(row['vector'])\n",
    "            fvector = dict(counts)\n",
    "            matched_str = None\n",
    "            packer_list = {}\n",
    "            for p,m in mutual_dict.items():\n",
    "                # Compare with train\n",
    "                #result = pool.apply_async(taskMiCal, (fvector,tested,m, threshold,))\n",
    "                inp = {'fvector': fvector, 'packer': p,\n",
    "                       'tested': tested, 'm': m, 'threshold':threshold}\n",
    "                result = self.taskMiCal_t(inp)\n",
    "                if result is not None:\n",
    "                    if result in packer_list:\n",
    "                        packer_list[result] += 1\n",
    "                    else:\n",
    "                        packer_list[result] = 1\n",
    "            if len(packer_list) > 0:\n",
    "                predict = max(packer_list.items(), key=operator.itemgetter(1))[0]\n",
    "                output.append(\"%s:%s:%s:%s\" % (assets, data_type, predict, row['vector_hash']))\n",
    "                if self.cacheFlg:\n",
    "                    if self.checkType == 0: # cache assets\n",
    "                        self.cache[row['vector_hash']] = predict\n",
    "                        if len(self.cache) % 10000 == 0:\n",
    "                            self.storeCache()\n",
    "                    else:\n",
    "                        self.appCache[row['vector_hash']] = predict\n",
    "                # input_list.append(inp)\n",
    "            #print(\"Input list: %d\" % len(input_list))\n",
    "            #task_thread = MultiTask(taskMiCal)\n",
    "            #result = task_thread.run(input_list)\n",
    "            #if result is not None:\n",
    "            #    packer_str = \"\"\n",
    "            #    for res in result:\n",
    "            #        if res is not None:\n",
    "            #            if packer_str != \"\":\n",
    "            #                packer_str += \";\"\n",
    "            #            packer_str += res.split(\":\")[0]\n",
    "            #    output.append(\"%s:%s:%s\" % (assets, data_type, packer_str))\n",
    "        return output\n",
    "\n",
    "    def appCheck2(self, app_df, app_md5, mutual_dict, threshold):\n",
    "        output = []\n",
    "        asset_vector = []\n",
    "        packer_vector = []\n",
    "        type_vector = []\n",
    "        vector_hash = []\n",
    "        assets = None\n",
    "        data_type = None\n",
    "        output = self.checkRow(app_df, mutual_dict, threshold)\n",
    "        for line in output:\n",
    "            asset_vector.append(line.split(\":\")[0])\n",
    "            type_vector.append(line.split(\":\")[1])\n",
    "            packer_vector.append(line.split(\":\")[2])\n",
    "            vector_hash.append(line.split(\":\")[3])\n",
    "        # Put into dataframe\n",
    "        if len(packer_vector) > 0:\n",
    "            packer_counter = Counter(packer_vector)\n",
    "            most, c = packer_counter.most_common()[0]\n",
    "        else:\n",
    "            most = \"None\"\n",
    "        row = {\n",
    "            'app_md5': app_md5, \n",
    "            'assets_vector': asset_vector, \n",
    "            'packer_vector': packer_vector, \n",
    "            'type_vector': type_vector, \n",
    "            'tested': app_df.iloc[0]['packer'],\n",
    "            'predict': most,\n",
    "            'vector_hash': vector_hash\n",
    "        }\n",
    "        print(row)\n",
    "        return row\n",
    "\n",
    "    def appCheck(self, input_df, mutual_dict, threshold):\n",
    "        ## Check based on app_md5, recommend on single test\n",
    "        from IPython.display import clear_output\n",
    "        total_app = len(input_df['app_md5'].unique())\n",
    "        pbar = tqdm(total=total_app)\n",
    "        if self.cacheFlg:\n",
    "            self.loadCache()\n",
    "        count = 0\n",
    "        out_df = pd.DataFrame()\n",
    "        for app in input_df['app_md5'].unique():\n",
    "            print(\"Calculate For App: %s\" % app)\n",
    "            app_df = input_df[input_df['app_md5'] == app]\n",
    "            print(\"Assets number: %d\" % len(app_df.md5.unique()))\n",
    "            if len(app_df.md5.unique()) > 40:\n",
    "                continue\n",
    "            if self.cacheFlg:\n",
    "                self.loadAppCache(app)\n",
    "            row = self.appCheck2(app_df, app, mutual_dict, threshold)\n",
    "            out_df = out_df.append(row, ignore_index=True)\n",
    "            # Store\n",
    "            if self.cacheFlg:\n",
    "                self.storeAppCache(app)\n",
    "            if count % 5 == 0:\n",
    "                clear_output()\n",
    "            pbar.update(1)\n",
    "            count += 1\n",
    "            if count > 0:\n",
    "                break\n",
    "        pbar.close()\n",
    "        return out_df\n",
    "    \n",
    "    def assetsCheck(self, input_df, mutual_dict, threshold):\n",
    "        import json\n",
    "        from IPython.display import clear_output\n",
    "\n",
    "        if self.cacheFlg:\n",
    "            self.loadCache()\n",
    "        ## Check based on vector_hashes, recommend for batch\n",
    "        total_assets = len(input_df['vector_hash'].unique())\n",
    "        pbar = tqdm(total=total_assets)\n",
    "        count = 0\n",
    "        out_df = pd.DataFrame()\n",
    "        for app in input_df['vector_hash'].unique():\n",
    "            app_df = input_df[input_df['vector_hash'] == app]\n",
    "            row = self.appCheck2(app_df, app, mutual_dict, threshold)\n",
    "            out_df = out_df.append(row, ignore_index=True)\n",
    "            pbar.update(1)\n",
    "            count += 1\n",
    "            if count % 50 == 0:\n",
    "                clear_output()\n",
    "        pbar.close()\n",
    "        if self.cacheFlg:\n",
    "            self.storeCache()\n",
    "        \n",
    "        return out_df\n",
    "\n",
    "    def classify(self, input_df, mutual_dict, threshold=10, assetCheck=False):\n",
    "        from IPython.display import clear_output\n",
    "\n",
    "        out_df = pd.DataFrame()\n",
    "        if assetCheck:\n",
    "            # ONLY RUN THIS ONE FOR GENERATE THE ASSETS HASHES (CACHE)\n",
    "            self.checkType = 0\n",
    "            out_df = self.assetsCheck(input_df, mutual_dict, threshold)\n",
    "        else:\n",
    "            self.checkType = 1\n",
    "            out_df = self.appCheck(input_df, mutual_dict, threshold)\n",
    "        return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "51906fc323701065580c9540289a1ce09033766a"
   },
   "outputs": [],
   "source": [
    "def statistic(packer_test_df, normal_test_df=None):\n",
    "    p_np = packer_test_df[packer_test_df['predict'] == \"None\"]  # Packer - non predict\n",
    "    p_p = packer_test_df[packer_test_df['predict'] != \"None\"]  # Packer - predicted\n",
    "    if normal_test_df is not None:\n",
    "        n_np =normal_test_df[normal_test_df['predict'] == \"None\"]   # Normal - non predict\n",
    "        n_pp = normal_test_df[normal_test_df['predict'] != \"None\"]   # Normal - predicted\n",
    "        print(\"Normal In Test: %d\" % len(normal_test_df.index))\n",
    "        print(\"Correct Normal Apps: %d \" % len(n_np.index))\n",
    "        normal_assets=0\n",
    "        for i,r in n_np.iterrows():\n",
    "            normal_assets += len(r['assets_vector'])\n",
    "\n",
    "    wrong_predict = packer_test_df[packer_test_df['predict'] != packer_test_df['tested']]\n",
    "    wrong_value_predict = wrong_predict[wrong_predict['predict'] != \"None\"]\n",
    "    correct_predict = packer_test_df[~packer_test_df.app_md5.isin(wrong_predict.app_md5)]\n",
    "    #UPX_df = wrong_predict[wrong_predict['tested'].str.contains(\"UPX\")]\n",
    "\n",
    "    print(\"Packer In Test: %d\" % len(packer_test_df.index))\n",
    "    print(\"Correct Predict: %d\" % len(correct_predict.index))\n",
    "    print(\"Wrong Predict: %d\" % len(wrong_predict.index))\n",
    "    print(\" + Wrong Value Predict: %d\" % len(wrong_value_predict.index))\n",
    "    print(\" + Missed: %d\" % len(p_np.index))\n",
    "    #print(\" + Wrong Predict (UPX App): %d\"  % len(UPX_df.index))\n",
    "\n",
    "    wrong_app = 0\n",
    "\n",
    "    for app in wrong_predict['app_md5'].unique():\n",
    "        app_df = wrong_predict[wrong_predict['app_md5'] == app]\n",
    "        wrong_app += 1\n",
    "\n",
    "    count_correct_assets=0\n",
    "    count_incorrect_assets=0\n",
    "\n",
    "    for i,r in correct_predict.iterrows():\n",
    "        count_correct_assets += len(r['assets_vector'])\n",
    "    for i,r in wrong_predict.iterrows():\n",
    "        count_incorrect_assets += len(r['assets_vector'])\n",
    "\n",
    "    print(\"Correct assets: %d\" % count_correct_assets)\n",
    "    print(\"Incorrect assets: %d\" % count_incorrect_assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "49dc7878a048f137a55332edd07dff6e695d7778",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/195 [02:00<6:29:08, 120.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# CODE - OUTPUT CLASSIFICATION RESULT BASED ON MUTUAL INFORMATION AND TEST DF\n",
    "# TOOK 6HOURS TO RUN FOR 34067 ASSETS AROUND 1-3s EACH ASSETS\n",
    "#PACKER_TEST_DF = os.path.join(ASSETS_DIR, \"packer_test_df_output_27012019_v5.pickle\")\n",
    "#packer_test_df = pd.DataFrame()\n",
    "#if os.path.isfile(PACKER_TEST_DF):\n",
    "#    with open(PACKER_TEST_DF, \"rb\") as fd:\n",
    "#        packer_test_df = pd.read_pickle(fd)\n",
    "#else:\n",
    "    ## CACHE DUPLICATED TEST \n",
    "    #mi_obj = MutualInformationClassification()\n",
    "    #dup_test_df = mi_obj.classify(reduced_test_df, mutual_dict, 10, True)\n",
    "    \n",
    "mi_obj = MutualInformationClassification()\n",
    "packer_test_df = mi_obj.classify(merged_test_df, mutual_dict, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKER_TEST_DF = os.path.join(ASSETS_DIR, \"packer_test_df_output_27012019_v5.pickle\")\n",
    "packer_test_df.to_pickle(PACKER_TEST_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packer In Test: 1\n",
      "Correct Predict: 0\n",
      "Wrong Predict: 1\n",
      " + Wrong Value Predict: 0\n",
      " + Missed: 1\n",
      "Correct assets: 0\n",
      "Incorrect assets: 0\n"
     ]
    }
   ],
   "source": [
    "statistic(packer_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "72385e21aa44f3e604a5c070c81de9b089949fd3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wrong_predict = packer_test_df[packer_test_df['predict'] != packer_test_df['tested']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6f80294a46ee012c8c4011af589a45fbeb51f97"
   },
   "source": [
    "# Test on Normal dataset\n",
    "final_normal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "9e74183d3335f4e4c8ac81ef996216444a1c5acf"
   },
   "outputs": [],
   "source": [
    "#if os.path.isfile(os.path.join(ASSETS_DIR, \"normal_test_df_27012019_good.pickle\")):\n",
    "#    with open(os.path.join(ASSETS_DIR, \"normal_test_df_27012019_good.pickle\"), \"rb\") as fd:\n",
    "#        normal_test_df = pd.read_pickle(fd)\n",
    "#else:\n",
    "final_normal_df = feature_model.refineDF(final_normal_df)\n",
    "final_normal_df = feature_model.createHashVector(final_normal_df)\n",
    "NORMAL_TEST_DF = os.path.join(ASSETS_DIR, \"normal_test_df_output_27012019_v5.pickle\")\n",
    "if os.path.isfile(NORMAL_TEST_DF):\n",
    "    with open(NORMAL_TEST_DF, \"rb\") as fd:\n",
    "        normal_test_df = pd.read_pickle(fd)\n",
    "else:\n",
    "    mi2_obj = MutualInformationClassification()\n",
    "    mi2_obj.unsetCache()\n",
    "    normal_test_df = mi2_obj.classify(final_normal_df, mutual_dict, 10)\n",
    "    \n",
    "md5_normal_list = []\n",
    "with open(os.path.join(BASE_DIR, \"md5_list_normal_integrated.pickle\")) as data_file:    \n",
    "        md5_normal_list = json.load(data_file)\n",
    "\n",
    "count_none = 0\n",
    "count_packer = 0\n",
    "for md5 in md5_normal_list:\n",
    "    target_normal_df = normal_test_df[normal_test_df['app_md5'] == md5]\n",
    "    if target_normal_df.empty:\n",
    "        count_none += 1\n",
    "    else:\n",
    "        count_packer += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "38ef0693a3e486fd21fd940dac7974062e845535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packer In Test: 1\n",
      "Correct Predict: 0\n",
      "Wrong Predict: 1\n",
      " + Wrong Value Predict: 0\n",
      " + Missed: 1\n",
      "Correct assets: 0\n",
      "Incorrect assets: 0\n",
      "Normal Dataset: 201\n",
      "  + Non-packer: 194 \n",
      "  + Packer: 7 \n",
      "Packer Accuracy: 96.8543046357616\n",
      "Packer Precision: 95.8974358974359\n",
      "Normal Precision: 96.51741293532339\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "statistic(packer_test_df)\n",
    " \n",
    "# We got the number from previous steps (statistic), the algorithm to got this one is from the paper\n",
    "\n",
    "tn = 398 # True Neg\n",
    "fn = 11 # False Neg\n",
    "tp = 187 # True Pos\n",
    "fp = 8 # False Pos\n",
    "print(\"Normal Dataset: %d\" % len(md5_normal_list))\n",
    "print(\"  + Non-packer: %d \" % count_none)\n",
    "print(\"  + Packer: %d \" % count_packer)\n",
    "p_accuracy = ((tn + tp) / (409+195))*100\n",
    "p_precision = (tp/195)*100\n",
    "print(\"Packer Accuracy: %s\" % p_accuracy)\n",
    "print(\"Packer Precision: %s\" % p_precision)\n",
    "\n",
    "n_precision = (count_none/201)*100\n",
    "print(\"Normal Precision: %s\" % n_precision)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae7995ede93fb295b43d504c256fca5a35479e07"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6fd27d11a5bedf2fe17b627e811670f4b675e014"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5379e5579fb33c6a2616cef33fab15b8e9765204"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
